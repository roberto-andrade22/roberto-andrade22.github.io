{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/roberto/Documents/GitHub/loan_default_classification/')\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Portfolio](index)\n",
    "\n",
    "---\n",
    "## **Predicting customer defaulting on a loan: Home Credit Group classification algorithms analysis**\n",
    "\n",
    "### Summary\n",
    "\n",
    "Given Home Credit's default risk data, this project aims to build a classification algorithm that will predict whether or not a client will default on a loan. Secondly, due to the business nature of the problem, the project also aims to analyze different models to try and explain their financial implications when optimizing for different metrics. Finally, the project will aim to not only optimize statistical performance but also computing power efficiency: a marginal improvement in accuracy may not be an overall gain given the time the model takes to perform.\n",
    "\n",
    "As the objective of the project is only to predict, at no point will we touch on output interpretability nor try to point out the most prominent features that drive defaulting on a loan.\n",
    "\n",
    "In order to accomplish this we will use Python and in particular Scikit learn 1.1.3 for both data processing as well as model implementation. For Gradient Boosted Trees, we will use the LightGBM package.\n",
    "\n",
    "Because we do know that the data is very imbalanced (over 90% of the observations are negative), the performance metric of interest will not be accuracy as is: just by always predicting negative we would be right most of the time. Instead we will focus on ROC Area Under the Curve, which is a far more balanced metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted:\t\t24825\n",
      "Did not default:\t282686\n"
     ]
    }
   ],
   "source": [
    "## Confusion matrix\n",
    "df = pd.read_csv('aggregated_train_data.csv', index_col= 0)\n",
    "df.rename(columns = {col: col.lower() for col in df.columns.values}, inplace = True)\n",
    "X = df.drop(columns=['target','sk_id_curr'])\n",
    "y = df.target.copy()\n",
    "print(f\"Defaulted:\\t\\t{y.value_counts()[1]}\")\n",
    "print(f\"Did not default:\\t{y.value_counts()[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is important to use such a metric which combines elements from both the true positive rate as well as the true negative rate: if we were to flag accurately all the defaulting loans (recall) we may avoid the costs of the default, however at the expense of missing on plenty of customers who would have paid their loans and interest back. As we do not know the associated benefits and costs of these two cases, ROC Area Under the Curve is the best we can do for this imbalanced dataset. We will also keep track of accuracy and runtime of each model, just not trying to optimize it as this could be done on the best model when deploying on the cloud if its performance warrants it.\n",
    "\n",
    "These were the models that were fitted:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Lasso Regression (L1)\n",
    "3. Support Vector Machines\n",
    "4. Decision Trees\n",
    "\n",
    "Ensemble Models<br>\n",
    "\n",
    "5. Random Forest\n",
    "6. LightGBM (Gradient Boosted Algorithm)\n",
    "\n",
    "These were the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.790970</td>\n",
       "      <td>0.920126</td>\n",
       "      <td>1788.541508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.758290</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>1005.142762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.752688</td>\n",
       "      <td>0.915304</td>\n",
       "      <td>12.469002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_full</th>\n",
       "      <td>0.696794</td>\n",
       "      <td>0.909928</td>\n",
       "      <td>30.610333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.617587</td>\n",
       "      <td>0.897792</td>\n",
       "      <td>78.623214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_l1</th>\n",
       "      <td>0.481147</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>33.661335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           roc_auc  accuracy      runtime\n",
       "LGBM                      0.790970  0.920126  1788.541508\n",
       "random_forest             0.758290  0.919271  1005.142762\n",
       "SVC                       0.752688  0.915304    12.469002\n",
       "logistic_regression_full  0.696794  0.909928    30.610333\n",
       "decision_tree             0.617587  0.897792    78.623214\n",
       "logistic_regression_l1    0.481147  0.919271    33.661335"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.read_csv('performance_metrics.csv', index_col = 0)\n",
    "metrics.sort_values(by = 'roc_auc', ascending= False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring runtime for a second, it can be clearly seen that Lightweight Gradient Boosting Model (LGBM) results are the clear winners: The model captures effectively those clients who will default while not rejecting those who would not default if given the loan better than any other model.\n",
    "\n",
    "This would be a massive improvement over the default of assuming every client will pay back: it will be as accurate as that one, but better in that this one will actually avoid a lot of the costs associated with a client defaulting. The other default assumption would be to assume that no client would ever pay back and not hand out any loans, which of course would result in the bank closing. Deploying this model would strike a great balance which would allow the bank to collect interest and principal on the loans they give while reducing the risk of giving loans that will not be paid back.\n",
    "\n",
    "The model does have some limitations and setbacks that should be addressed when implementing on a larger scale.\n",
    "\n",
    "* Ideally, some more feature engineering should be conducted to further feed information to the model as to how defaulting really works.\n",
    "* Because of the large nature of the data, this model is likely not perfectly tuned: when getting access to the adequate computing power, the model can and should be modified to improve performance.\n",
    "* Speaking about computing power, given that we did not have access to GPU resources, we did not try to implement more powerful models, such as XGBoost. That model would very likely perform better than LGBM, and should be tried if possible. Any improvement, even if marginal, would implicate massive savings in costs for the bank when scaled to thousands of transactions.\n",
    "\n",
    "### References\n",
    "\n",
    "* [Data](https://www.kaggle.com/competitions/home-credit-default-risk/data)\n",
    "* [LightGBM](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier)\n",
    "* [Full Project](https://github.com/roberto-andrade22/loan_default_classification)\n",
    "\n",
    "---\n",
    "[Back to Portfolio](index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
